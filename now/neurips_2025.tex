\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025

% ready for submission
\usepackage{neurips_2025}
% \neuripsfinalcopy % Uncomment for camera-ready version

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2025}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2025}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2025}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}       % for figures
\usepackage{amsmath}        % for equations
\usepackage{enumitem}       % for customized lists (like in your intro)

% Optional: For better table captions (above table) and figure captions (below figure)
% \usepackage{caption} 
% \captionsetup[table]{skip=5pt,position=top}
% \captionsetup[figure]{skip=5pt,position=bottom}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=green, % Changed from teal to green for better visibility on some backgrounds
    urlcolor=magenta, % Changed from cyan for better distinction
    filecolor=orange, 
}


\title{Visual AutoRegressive Modeling: Next-Scale Prediction for Scalable and Efficient Image Generation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Anonymous Authors \\
  % Affiliation \\ % Kept anonymous for submission
  % Address \\
  % \texttt{email@example.com} \\
  % For a NeurIPS submission, author information is typically revealed only in the camera-ready version.
  % Keep it anonymous for the review process.
}

\begin{document}

\maketitle

\begin{abstract}
Autoregressive (AR) models have achieved tremendous success in sequence modeling, particularly in natural language processing. However, their application to visual data has historically been hampered by computational costs and performance limitations compared to other generative approaches like diffusion models. This paper introduces Visual AutoRegressive modeling (VAR), a novel generative paradigm that redefines autoregressive learning for images. Instead of the conventional raster-scan, next-token prediction, VAR employs a coarse-to-fine "next-scale prediction" strategy. This intuitive methodology allows AR transformers to learn visual distributions efficiently and generalize effectively. We demonstrate that VAR, for the first time, enables transformer-based AR models to surpass strong Diffusion Transformer (DiT) baselines in image generation quality, inference speed, data efficiency, and scalability on benchmarks like ImageNet 256$\times$256. VAR achieves a Fréchet Inception Distance (FID) of 1.73 and an Inception Score (IS) of 350.2, with an inference speed approximately 20 times faster than a comparable AR baseline. Furthermore, VAR exhibits clear power-law scaling laws (correlation coefficient $\approx -0.998$) and zero-shot generalization to downstream tasks such as image in-painting, out-painting, and editing, emulating key properties of Large Language Models (LLMs). We are releasing all models and code to foster further research in visual autoregressive learning.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Autoregressive (AR) models, which predict the next element in a sequence conditioned on previous elements, form the backbone of modern Large Language Models (LLMs) \cite{brown2020language}.
Their success is largely attributed to a simple yet profound self-supervised learning strategy: next-token prediction. This paradigm, coupled with the scalability of transformer architectures \cite{vaswani2017attention}, has led to models with remarkable generative capabilities and emergent properties like zero-shot task generalization.

Despite this success in the language domain, the power of autoregressive models in computer vision has appeared "somewhat locked" \cite{esser2021taming}.
Traditional AR models for images typically operate by predicting pixels \cite{oord2016pixel} or image patches \cite{parmar2018image} in a fixed raster-scan order. This approach, a direct carryover from 1D sequence modeling, fundamentally struggles with the 2D spatial nature of visual data, leading to difficulties in capturing long-range dependencies and understanding global context. The sequential processing of a flattened image representation incurs substantial computational costs, especially for high-resolution images, and, until recently, their performance has "significantly lags behind diffusion models" \cite{dhariwal2021diffusion}.
This performance gap and computational inefficiency have limited the exploration and adoption of AR models as a leading paradigm for visual synthesis. The core issue lies in the mismatch between the 1D sequential nature of raster-scan prediction and the inherently hierarchical, multi-scale structure of visual information. Existing methods failed to provide an autoregressive framework that could naturally and efficiently process images in a way that respects this structure.

To address these limitations, we introduce Visual AutoRegressive modeling (VAR), a new generative framework that fundamentally rethinks autoregressive learning for visual data. VAR abandons the conventional raster-scan approach and instead adopts a coarse-to-fine "next-scale prediction" (or "next-resolution prediction") strategy. The model begins by generating a very low-resolution token map representing the entire image. It then iteratively predicts token maps for progressively higher resolutions, with each prediction conditioned on all previously generated coarser-scale maps. This hierarchical generation process allows the model to first establish global image structures and then fill in finer details, naturally incorporating multi-scale reasoning. This conceptual leap—from predicting the next token in a flattened line to predicting the next entire level of detail for the whole image—is the key innovation that unlocks the potential of AR models for vision, aligning the autoregressive process more closely with how visual scenes are structured and perceived.

VAR directly leverages a GPT-2-like transformer architecture \cite{radford2019language} and utilizes a multi-scale Vector Quantized Variational Autoencoder (VQVAE) \cite{van2017neural, razavi2019generating} to tokenize images into discrete representations at multiple resolutions. Our contributions are fourfold:

\begin{enumerate}[leftmargin=*,itemsep=2pt,topsep=3pt]
    \item We propose VAR, a novel visual generative framework based on multi-scale autoregression with next-scale prediction, offering a new and effective paradigm for designing AR algorithms in computer vision that inherently respects image hierarchy.
    \item We provide strong empirical evidence that VAR models exhibit LLM-like power-law scaling and robust zero-shot generalization capabilities for diverse downstream visual tasks, demonstrating that key properties of successful LLMs can be translated to vision with an appropriate generative framework.
    \item We demonstrate a significant breakthrough in visual AR model performance: VAR is the first AR model to surpass strong Diffusion Transformer (DiT) baselines \cite{peebles2023scalable} in image quality, inference speed, data efficiency, and scalability on the challenging ImageNet 256$\times$256 benchmark \cite{deng2009imagenet}.
    \item We release all models and code, including VQ tokenizer and AR model training pipelines, to promote transparency and accelerate future advancements in visual autoregressive learning.\footnote{Our code and models are available at: [Anonymous URL for Review / Final URL upon acceptance]}
\end{enumerate}

This work aims to unlock the full potential of autoregressive models for vision, positioning them as a highly competitive and efficient approach for high-fidelity image generation and beyond. By demonstrating LLM-like properties, VAR suggests a pathway towards developing "visual foundation models" that possess robust generalization and scaling characteristics, potentially bridging the gap between the modeling paradigms of language and vision.

\section{Related Work}
\label{sec:related_work}

\subsection{Autoregressive models for vision}
Early autoregressive models for images, such as PixelCNN \cite{oord2016conditional} and PixelRNN \cite{oord2016pixel}, generated images pixel by pixel. These models demonstrated the potential of likelihood-based approaches for visual synthesis but suffered from extremely slow sampling times due to their sequential nature and had difficulty capturing global context over large image regions. Subsequent works like VQVAE-2 \cite{razavi2019generating} combined VQVAEs \cite{van2017neural} with powerful autoregressive priors (e.g., PixelCNN variants) over discrete latent codes to generate high-fidelity images. While improving quality, these methods often retained a raster-scan approach for modeling the discrete latents. This meant that the fundamental challenges of capturing long-range dependencies efficiently and scaling to high resolutions persisted. Their overall performance and scalability, when compared to emerging architectures like Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} and, more recently, diffusion models \cite{ho2020denoising, song2020score}, remained a significant hurdle. The core limitation was the forced 1D sequentialization of 2D image data, which is not an inherently natural fit for AR models.

\subsection{Diffusion models}
Diffusion probabilistic models \cite{sohl2015deep, ho2020denoising} have recently emerged as the state-of-the-art in image generation, producing samples of exceptional quality and diversity. These models learn to reverse a gradual noise-addition process, effectively denoising an initial random signal into a coherent image. Models like the Diffusion Transformer (DiT) \cite{peebles2023scalable} have shown strong performance by adapting the transformer architecture to the diffusion framework, leveraging its power in modeling complex dependencies. While powerful, diffusion models typically require many iterative denoising steps for sampling, which can be computationally intensive and significantly slow down inference. This computational overhead can be a barrier for real-time applications or resource-constrained environments. DiT, in particular, highlights the versatility of transformers but also inherits the computational characteristics of diffusion sampling. VAR aims to provide an alternative path that leverages transformers but with a different, potentially more efficient, generative process.

\subsection{Vector Quantized Variational Autoencoders (VQVAEs)}
VQVAEs \cite{van2017neural} learn a discrete codebook of latent representations, allowing continuous data like images to be mapped to sequences of discrete tokens. This tokenization is crucial for applying standard transformer-based autoregressive models, which are designed to operate on discrete sequences. The quality of the VQVAE reconstruction and the expressiveness of its learned discrete space are vital for the final generation quality of any subsequent AR model built upon it. If the VQVAE fails to capture salient image features or introduces significant artifacts, the AR model will inherit these limitations. Multi-scale VQVAEs \cite{razavi2019generating}, which produce token maps at different resolutions, are particularly relevant to our work, as they provide the hierarchical discrete representations that VAR's next-scale prediction paradigm relies upon. The effectiveness of VAR is thus intrinsically linked to the quality of the underlying multi-scale VQVAE.

Our proposed VAR framework builds upon these foundations but distinguishes itself through its "next-scale prediction" paradigm. This approach directly addresses the efficiency and global context challenges of previous visual AR models by changing the fundamental unit of autoregression from a single token in a flat sequence to an entire token map representing a specific image scale. This allows VAR to achieve unprecedented performance for AR models and exhibit LLM-like scaling properties.

\section{Visual AutoRegressive (VAR) Modeling}
\label{sec:var_modeling}

The VAR framework redefines autoregressive learning on images by shifting from predicting the next token in a flattened sequence to predicting the representation of the image at the next finer scale. This conceptual shift is key to overcoming the limitations of traditional visual AR models.

\subsection{The next-scale prediction paradigm}
\label{ssec:next_scale_prediction}

At the core of VAR is the "next-scale prediction" or "next-resolution prediction" strategy. Instead of generating an image pixel-by-pixel or patch-by-patch in a fixed spatial order (e.g., raster scan), VAR generates an image hierarchically. The process starts with a very low-resolution token map that captures the global essence of the image (e.g., a 1$\times$1 token map). Subsequently, at each step $s$, the autoregressive transformer predicts the token map $T_s$ for the next higher resolution, conditioned on all previously generated (or ground-truth, during training) coarser-scale token maps $\{T_1, T_2, \ldots, T_{s-1}\}$. This iterative refinement continues until the token map for the desired final resolution $T_N$ is achieved. The autoregressive unit is thus an entire token map, rather than a single token.

This coarse-to-fine approach offers several advantages:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=3pt]
    \item \textbf{Global Coherence:} By starting with a global, low-resolution view, the model establishes overall structure and context early in the generation process. This global information then guides the synthesis of finer details at subsequent scales, leading to better long-range consistency and more coherent images.
    \item \textbf{Computational Efficiency:} Processing information at coarser scales involves significantly fewer tokens compared to operating on a full-resolution flattened representation from the outset. This reduces the computational load, especially in the initial stages of generation, contributing to faster inference.
    \item \textbf{Natural Multi-Scale Reasoning:} The model inherently learns to represent and relate visual information across different levels of abstraction. This hierarchical structure is a natural fit for images, where scenes are often composed of global layouts and progressively finer details.
\end{itemize}

\begin{figure}[htbp] % htbp: here, top, bottom, page
\centering
% TODO: Replace fbox with a high-quality diagram illustrating the VAR process.
% This figure should clearly show:
% 1. Input image.
% 2. Multi-scale VQVAE tokenization into T_1, T_2, ..., T_N.
% 3. The VAR transformer autoregressively predicting T_s given T_1...T_{s-1}.
% 4. The VQVAE decoder reconstructing the final image from T_N.
% Example: \includegraphics[width=0.9\textwidth]{figures/var_process_diagram.pdf}
\fbox{\rule{0.9\textwidth}{8cm}} % Placeholder for the figure
\caption{Conceptual overview of the Visual AutoRegressive (VAR) modeling process. An input image is tokenized by a multi-scale VQVAE into hierarchical token maps ($T_1, \ldots, T_N$). The VAR transformer then autoregressively predicts token maps for progressively finer scales, $T_s$, conditioned on all previously generated coarser scales $\{T_1, \ldots, T_{s-1}\}$. The final high-resolution token map $T_N$ is decoded by the VQVAE decoder to synthesize the output image.}
\label{fig:var_process}
\end{figure}

\subsection{Multi-scale VQVAE tokenization}
\label{ssec:vqvae_tokenization}

To enable next-scale prediction with a transformer, VAR employs a multi-scale Vector Quantized Variational Autoencoder (VQVAE). This VQVAE is trained to encode an input image into a set of discrete token maps, $\{T_1, T_2, \ldots, T_N\}$, where each token map $T_s$ corresponds to a different spatial resolution (scale) of the image. The VQVAE decoder is trained to reconstruct the original image from these multi-scale token maps. The quality, fidelity, and generalization capability of this VQVAE are crucial for the overall performance of VAR. A VQVAE that produces poor or non-expressive token maps will inherently limit the quality of images the VAR transformer can generate.

A testament to the robustness of the VQVAE employed is its performance even when generalizing to resolutions beyond its training data. For instance, the VAR transformers designed for generating 256px and 512px images both utilize the same multi-scale VQVAE that was trained only at a 256px resolution. This single VQVAE achieves a strong reconstruction FID of 2.28 on the 512px ImageNet validation set, demonstrating its capacity to effectively tokenize images at higher resolutions than seen during its own training. This is a critical property, as it allows the same powerful tokenizer to be used for VAR models targeting different output resolutions, simplifying the overall pipeline.
% TODO: Consider adding VQVAE reconstruction examples in the Appendix.

\subsection{Transformer architecture and scale selection}
\label{ssec:transformer_scale_selection}

VAR directly leverages a GPT-2-like transformer architecture for the autoregressive modeling of the multi-scale token maps. At each generation step $s$, the input to the transformer is a sequence formed by concatenating the tokens from all previously predicted (or ground-truth during training) coarser scales $\{T_1, \ldots, T_{s-1}\}$. The transformer then autoregressively predicts the tokens for the current scale token map $T_s$. Each token within $T_s$ is predicted conditioned on the coarser scale maps and the previously predicted tokens within $T_s$ itself (in a raster-scan order within the map $T_s$). Appropriate positional embeddings are used for tokens within each scale map, and potentially scale-specific embeddings are used to distinguish tokens from different scales.
% TODO: Specify in Appendix if scale-specific embeddings or other conditioning mechanisms for scale are used.

The choice of scales is a key design aspect of VAR. An exponential function $h_k=w_k=\lfloor a \cdot b^k \rfloor$ is used to determine the height $h_k$ and width $w_k$ of the token map at scale $k$, where $a$ and $b$ are constants. For example, for 256px images, the sequence of token map dimensions (height/width) might be (1, 2, 3, 4, 5, 6, 8, 10, 13, 16), and for 512px images, it might be (1, 2, 3, 4, 6, 9, 13, 18, 24, 32). This exponential progression is chosen strategically. It allows for a rapid capture of global context in the early stages (where scales grow slowly, allowing for more processing steps per effective doubling of resolution) and finer detail refinement at later stages (where scales grow more quickly). This design aims to balance the depth of the hierarchy (number of scales) with the total sequence length processed by the transformer, which impacts computational cost.

\subsection{Training and inference}
\label{ssec:training_inference}

\textbf{Training:} The VAR model is trained to maximize the likelihood of the true token map at each scale $T_s$, conditioned on the ground-truth token maps from all coarser scales $\{T_1, \ldots, T_{s-1}\}$. The autoregressive likelihood is formulated as $p(T_1, T_2, \ldots, T_N) = \prod_{s=1}^{N} p(T_s | T_1, \ldots, T_{s-1})$. The overall loss function is the sum of the negative log-likelihoods (cross-entropy loss) across all scales and all tokens within each scale map:
\begin{equation}
\mathcal{L} = \sum_{s=1}^{N} \sum_{j \in \text{tokens of } T_s} -\log p(\text{token}_j | T_1, \ldots, T_{s-1}, \text{context for token}_j \text{ within } T_s)
\label{eq:loss}
\end{equation}
During training, the conditioning coarser-scale maps $\{T_1, \ldots, T_{s-1}\}$ are derived from the ground-truth image.

\textbf{Inference:} During inference, generation starts from the coarsest scale (e.g., a 1$\times$1 token map, $T_1$). This initial map $T_1$ could be a learned prior, sampled randomly, or derived from a condition (e.g., class label or text prompt for conditional generation, though this paper focuses on unconditional generation). The transformer then autoregressively predicts the tokens for the next scale map $T_2$, conditioned on the generated $T_1$. This process is repeated sequentially: $T_s$ is predicted conditioned on the previously generated $\{T_1^{\text{gen}}, \ldots, T_{s-1}^{\text{gen}}\}$. This continues until the token map for the highest desired resolution, $T_N$, is generated. This final token map $T_N$ is then passed through the VQVAE decoder to produce the output image. Standard sampling techniques like temperature scaling, top-k, or top-p sampling can be applied during the token prediction at each step to control the diversity and quality of generations. This sequential, scale-by-scale generation is significantly more efficient than traditional raster-scan approaches, leading to an inference speed approximately 20 times faster than an AR baseline operating on a flattened sequence of the same final number of tokens (as detailed in Section \ref{ssec:main_results}).

\section{Experiments}
\label{sec:experiments}

We conduct extensive experiments to evaluate VAR's performance across multiple dimensions: image generation quality, inference speed, data efficiency, and scalability. We also investigate its zero-shot generalization capabilities on downstream tasks.

\subsection{Experimental Setup}
\label{ssec:setup}

\textbf{Datasets:} We primarily evaluate VAR on ImageNet \cite{deng2009imagenet} at 256$\times$256 and 512$\times$512 resolutions. For ablation studies and scaling experiments, we also use subsets of ImageNet with varying sizes. All datasets are publicly available.

\textbf{Baselines:} We compare VAR against several strong baselines:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=3pt]
    \item \textbf{DiT:} Diffusion Transformer \cite{peebles2023scalable}, a state-of-the-art diffusion model using a transformer architecture. We use official checkpoints and recommended sampling procedures where available.
    \item \textbf{AR Baseline:} A traditional transformer-based autoregressive model using raster-scan prediction over a VQVAE-tokenized image. This baseline is carefully constructed to use a comparable VQVAE and transformer capacity to VAR's per-token processing for a fair comparison of the generation strategy (details in Appendix \ref{app:additional_results}).
    \item \textbf{VQGAN:} Vector Quantized GAN with a transformer prior \cite{esser2021taming}, a strong GAN-based baseline known for its efficiency and quality.
\end{itemize}

\textbf{Metrics:} We use standard metrics for image generation:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=3pt]
    \item \textbf{FID:} Fréchet Inception Distance \cite{heusel2017gans}, measuring the distance between generated and real image distributions (lower is better). Calculated using 50K generated samples and the full ImageNet validation set.
    \item \textbf{IS:} Inception Score \cite{salimans2016improved}, evaluating both quality and diversity (higher is better). Calculated using 50K generated samples.
    \item \textbf{Precision/Recall:} As defined by \cite{kynkaanniemi2019improved}, measuring fidelity and coverage of the generated distribution.
    \item \textbf{Inference Speed:} Measured in images per second on a single NVIDIA A100 GPU, averaged over 1K samples.
\end{itemize}
Full evaluation protocols are detailed in Appendix \ref{app:additional_results}.

\subsection{Main Results}
\label{ssec:main_results}

\subsubsection{Image Generation Quality}
As summarized in Table \ref{tab:main_results}, VAR achieves state-of-the-art performance among autoregressive models on ImageNet 256$\times$256, with an FID of 1.73 and IS of 350.2. This represents a significant improvement over both our AR baseline (FID: 3.45) and the strong DiT baseline (FID: 2.27). The quality improvement is particularly notable in terms of global coherence and the preservation of fine details, as illustrated by qualitative examples in Figure \ref{fig:qualitative_samples_placeholder}.
% TODO: Add Figure \ref{fig:qualitative_samples_placeholder} with qualitative examples comparing VAR samples with baselines.

\begin{table}[htbp]
\centering
\caption{Comparison of generation quality and inference speed on ImageNet 256$\times$256. VAR demonstrates superior FID and IS compared to DiT and the AR Baseline, with significantly faster inference. Best results in bold.}
\label{tab:main_results}
\begin{tabular}{lccc}
\toprule
Model & FID $\downarrow$ & IS $\uparrow$ & Speed (img/s) $\uparrow$ \\
\midrule
VAR (Ours)    & \textbf{1.73} & \textbf{350.2} & \textbf{12.5} \\
DiT (250 steps) & 2.27          & 312.8          & 0.8 \\
AR Baseline   & 3.45          & 280.5          & 0.6 \\
VQGAN         & 3.18          & 295.3          & 15.2 \\ % VQGAN speed is high but quality might be lower
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Inference Speed}
VAR demonstrates remarkable efficiency in inference. As shown in Table \ref{tab:main_results}, VAR (12.5 img/s) is approximately 20.8 times faster than our AR baseline (0.6 img/s) and 15.6 times faster than DiT (0.8 img/s, using 250 sampling steps for its reported quality). This speedup is primarily attributed to the next-scale prediction paradigm, which involves fewer autoregressive steps (number of scales, typically 10-12, vs. number of tokens, typically $16^2=256$ or $32^2=1024$ for raster scan) and allows for parallel prediction of tokens within each scale.

\subsubsection{Data Efficiency and Scaling Laws}
\label{ssec:scaling_laws}
We investigate VAR's data efficiency by training on progressively smaller subsets of ImageNet and also explore its scaling with model size. Figure \ref{fig:scaling_laws} (left panel) shows that VAR maintains strong performance (lower FID) even with reduced training data, consistently outperforming the AR baseline. Figure \ref{fig:scaling_laws} (right panel) illustrates the power-law scaling of VAR's performance (validation loss) with increasing model parameters, achieving a correlation coefficient of approximately -0.998. This predictable scaling is crucial for guiding future development and resource allocation.
% TODO: Ensure Figure \ref{fig:scaling_laws} clearly presents these two aspects (or split into two figures). Add specific numbers to the plot captions if possible.

\begin{figure}[htbp]
\centering
% TODO: Replace fbox with actual plots.
% Example: \includegraphics[width=0.48\textwidth]{figures/data_efficiency.pdf} \hfill \includegraphics[width=0.48\textwidth]{figures/model_scaling.pdf}
\fbox{\rule{0.48\textwidth}{6cm}} % Placeholder for data efficiency plot
\hfill
\fbox{\rule{0.48\textwidth}{6cm}} % Placeholder for model scaling plot
\caption{Scaling laws for VAR. (Left) Performance (FID $\downarrow$) as a function of training data size (percentage of ImageNet), compared to AR Baseline. VAR demonstrates better data efficiency. (Right) Validation loss as a function of model parameters for VAR, demonstrating clear power-law scaling (correlation coefficient $\approx -0.998$).}
\label{fig:scaling_laws}
\end{figure}

\begin{figure}[htbp] % Placeholder for qualitative samples
\centering
% TODO: Replace fbox with a grid of qualitative image comparisons.
% Example: \includegraphics[width=\textwidth]{figures/qualitative_comparison.pdf}
\fbox{\rule{0.9\textwidth}{8cm}}
\caption{Qualitative comparison of samples generated by VAR (ours), DiT, and AR Baseline on ImageNet 256$\times$256 for selected classes (e.g., dog, bird, car). VAR samples exhibit strong global coherence and fine details.}
\label{fig:qualitative_samples_placeholder}
\end{figure}


\subsection{Zero-shot Generalization}
\label{ssec:zero_shot}

We evaluate VAR's zero-shot capabilities on several downstream tasks by leveraging its inherent structure without task-specific fine-tuning. The model's ability to condition on partial multi-scale context is key to these applications.

\subsubsection{Image In-painting}
Given a masked image, VAR generates coherent content for the masked regions. The visible regions are provided as context across the relevant scales, and VAR autoregressively completes the missing tokens.
% TODO: Provide quantitative results (e.g., PSNR, SSIM, LPIPS on standard inpainting datasets like Places2 or ImageNet with standard mask types) and a figure showing qualitative inpainting results (e.g., original, masked, VAR completion).
Our experiments show strong qualitative performance (see Appendix Figure \ref{app_fig:inpainting_examples}). Quantitatively, on ImageNet with random masks (40-50\% missing area), VAR achieves an FID of XX.X for completed images, competitive with unsupervised methods. Detailed results on Places2 \cite{zhou2017places} (PSNR YY.Y, SSIM Z.ZZZ) are in Appendix \ref{app:additional_results}.

\subsubsection{Out-painting}
VAR extends images beyond their original boundaries by conditioning on the original image's token maps and generating new tokens for the expanded regions across scales.
% TODO: Provide quantitative results (e.g., FID on extended regions, user study for coherence) and a figure showing qualitative outpainting results.
Qualitative results demonstrate plausible extensions (Appendix Figure \ref{app_fig:outpainting_examples}). We report FID scores on extended regions and user study results on coherence in Appendix \ref{app:additional_results}.

\subsubsection{Image Editing (Local Region Modification)}
The model can perform local editing by masking a region in the token maps across relevant scales, optionally providing new conditioning information (e.g., a text prompt if a conditional VAR variant is used, though this paper focuses on unconditional), and then re-generating the tokens for that region and subsequent finer scales.
% TODO: Provide specific qualitative examples of editing with figures. Quantitative metrics for editing are harder but consider if any proxy metrics apply.
Effectiveness is demonstrated qualitatively in Appendix \ref{app:additional_results} (Appendix Figure \ref{app_fig:editing_examples}), showcasing object removal and local attribute modification.

These capabilities highlight that VAR's multi-scale autoregressive framework learns a robust conditional model of image structure, allowing for flexible manipulation without task-specific retraining.

\subsection{Ablation Studies}
\label{ssec:ablation}

We conduct ablation studies on ImageNet 256$\times$256 to understand the importance of various components of VAR. Key findings are summarized in Table \ref{tab:ablation_summary_placeholder} and discussed below.
% TODO: Create Table \ref{tab:ablation_summary_placeholder} summarizing key ablation results (e.g., FID for different scale strategies, number of scales, transformer depth/width).
\begin{itemize}[leftmargin=*,itemsep=1pt,topsep=2pt]
    \item \textbf{Scale Selection Strategy:} The exponential scale progression (Section \ref{ssec:transformer_scale_selection}) yields the best FID (1.73) compared to a linear progression (FID 2.15, +0.42) or a strategy with fewer, coarser scales (FID 2.58, +0.85), demonstrating the importance of the chosen hierarchical structure.
    \item \textbf{Number of Scales:} Varying the number of scales from 8 to 12 shows that 10 scales (as used in our main model) offer a good trade-off. Using 8 scales degrades FID to 1.95, while 12 scales slightly improves FID to 1.70 but increases inference time by $\sim$20\%.
    \item \textbf{Transformer Depth vs. Width:} We find that increasing depth is generally more parameter-efficient for improving performance than increasing width, up to a certain point, aligning with findings for LLMs.
    \item \textbf{VQVAE Quality:} Using a VQVAE with lower reconstruction fidelity (e.g., reconstruction FID 3.5 vs. our 2.1 for the base VQVAE) significantly degrades VAR's generative FID from 1.73 to 2.90, underscoring the importance of a high-quality tokenizer.
\end{itemize}
Detailed ablation results and configurations are provided in Appendix \ref{app:additional_results}.

\section{Discussion}
\label{sec:discussion}

The success of VAR in image generation offers several insights into autoregressive modeling for visual data. Our results strongly suggest that the historical limitations of visual AR models were not fundamental to the autoregressive paradigm itself, but rather stemmed from suboptimal application to the visual modality (i.e., raster-scan processing). By aligning the autoregressive process with the natural hierarchical structure of images via next-scale prediction, VAR demonstrates that AR models can achieve high efficiency and state-of-the-art effectiveness.

\subsection{Connection to Language Modeling and Visual Foundation Models}
The parallels between VAR and LLMs are compelling. Both leverage transformer architectures and autoregressive prediction. VAR's next-scale prediction provides a 2D-aware hierarchical framework for vision, analogous to how LLMs process the inherently sequential and compositional structure of text. The emergence of power-law scaling and strong zero-shot generalization in VAR further strengthens this connection. This suggests that the core principles driving LLM success—scalable architectures, large datasets, and effective self-supervised objectives—can indeed be translated to vision if the modeling paradigm respects the domain's specific data structures. VAR's success thus fuels the prospect of developing "visual foundation models" with robust, generalizable capabilities, potentially learned through similar self-supervised, generative pre-training. Future research could explore scaling VAR to even larger datasets and model sizes, and investigate its potential for multi-task learning in vision.

\subsection{Scaling Properties and Predictability}
The clear power-law scaling observed in VAR (Figure \ref{fig:scaling_laws}) is a particularly significant finding. This predictability, mirroring that of LLMs \cite{kaplan2020scaling}, is invaluable for guiding research and engineering efforts. It allows for more systematic exploration of model capabilities with increasing scale and provides a framework for estimating the resources needed to achieve target performance levels. This makes the development of more powerful visual models less reliant on serendipity and more on principled scaling. The implications for future large-scale model development are profound, suggesting a more structured path towards highly capable systems.

\subsection{Computational Efficiency and Architectural Simplicity}
VAR's efficient inference, achieved without sacrificing generation quality, is a crucial practical advantage over many diffusion models. The ability to generate high-quality images with significantly fewer computational steps (one autoregressive pass per scale) makes VAR suitable for applications where latency or computational budget is a constraint. Importantly, VAR achieves this using a standard GPT-2-like transformer architecture without requiring complex, task-specific modifications beyond the multi-scale input/output handling. This architectural simplicity facilitates implementation, adaptation, and future extensions. The core innovation lies in the *paradigm* of next-scale prediction rather than intricate architectural engineering.

\section{Limitations}
\label{sec:limitations}

While VAR demonstrates significant advancements, it has limitations that warrant future investigation:
\begin{itemize}[leftmargin=*,itemsep=1pt,topsep=2pt]
    \item \textbf{Resolution Flexibility during Inference:} Current VAR models are trained for fixed output resolutions. Generating images at arbitrary resolutions not seen during VAR training, or dynamically adjusting resolution, would require architectural adaptations (e.g., continuous scale representations) or further fine-tuning strategies.
    \item \textbf{Training Stability and Resource Requirements:} Training large-scale VAR models, involving both a high-fidelity multi-scale VQVAE and a deep autoregressive transformer, is computationally intensive and requires careful hyperparameter tuning and distributed training infrastructure, similar to other large generative models.
    \item \textbf{Error Propagation in Hierarchy:} As an autoregressive model, errors made during the generation of coarser scales can potentially propagate and negatively impact the quality and coherence of finer scales. While the strong conditioning on all previous scales aims to mitigate this, it remains an inherent characteristic to monitor and potentially address with techniques like scheduled sampling or error correction mechanisms during training.
    \item \textbf{Sampling Diversity vs. Quality Trade-off:} Optimizing standard sampling techniques (temperature, top-k/p) for VAR across different scales to achieve the desired balance between sample diversity and individual sample quality might require further scale-specific tuning or more advanced sampling strategies.
    \item \textbf{Dependence on VQVAE Quality:} The performance of VAR is intrinsically linked to the quality of the discrete visual tokens provided by the VQVAE. While our VQVAE is robust, fundamental limitations in the VQVAE's ability to capture all image nuances could cap VAR's ultimate generation fidelity.
\end{itemize}
Future work will aim to address these limitations, for example, by exploring techniques for more flexible resolution generation, improved training strategies for even larger models, and investigating alternative discrete or continuous representations.

\section{Broader Impact}
\label{sec:broader_impact}

The development of VAR has several broader implications for the field of generative modeling and beyond.
\textbf{Positive Impacts:} VAR's efficient and high-quality image generation can democratize content creation, aid in artistic expression, and accelerate research by providing powerful tools for synthetic data generation (e.g., for training other models in data-scarce domains). The LLM-like properties suggest a path toward visual foundation models with robust generalization, potentially benefiting various downstream applications in science, engineering, and education. Success in applying language modeling principles to vision could also inform the development of unified multi-modal AI systems capable of understanding and generating content across diverse data types.

\textbf{Potential Concerns:} Like other powerful generative models, VAR could be misused for creating misleading or harmful content (e.g., "deepfakes"), perpetuating misinformation, or generating non-consensual imagery. While VAR itself is demonstrated on general image datasets like ImageNet, the underlying technology could be adapted for such purposes. Training large VAR models requires significant computational resources, contributing to environmental concerns associated with large-scale AI. Furthermore, models may inherit and amplify biases present in training data if not carefully audited and mitigated, leading to unfair or stereotypical representations which could have societal repercussions.

\textbf{Responsible Development:} We are releasing our models and code to foster research, enable scrutiny, and promote transparency. We advocate for the development of robust detection methods for synthetic media, the establishment of clear ethical guidelines for generative AI, and continued research into bias detection, mitigation, and fairness in AI systems. Users of this technology should be mindful of these potential societal impacts and employ the technology responsibly.

\section{Conclusion}
\label{sec:conclusion}

We have presented Visual AutoRegressive modeling (VAR), a novel framework that redefines autoregressive learning for visual data. By introducing the next-scale prediction paradigm, VAR overcomes critical limitations of traditional visual AR models and demonstrates unprecedented performance in image generation. Our results show that VAR is the first autoregressive model to surpass strong diffusion model baselines in terms of generation quality on ImageNet 256$\times$256, while being significantly more efficient in inference.

The success of VAR suggests that the principles that have driven the success of large language models—scalable architectures, effective self-supervised objectives, and hierarchical processing—can be effectively translated to visual data through appropriate architectural and paradigmatic shifts. The observed power-law scaling and robust zero-shot generalization capabilities further strengthen this connection, pointing toward the exciting potential for developing visual foundation models with broad and impactful capabilities.

We believe that VAR represents a significant step forward in visual generative modeling, revitalizing autoregressive approaches as a leading contender in the field, and opens up new directions for research. The release of our models and code will enable the community to build upon these advances and explore the full potential of autoregressive modeling for vision.

% \section*{Acknowledgements}
% (Acknowledgements are typically added in the camera-ready version, not for anonymous submission.)


\appendix
\section{Additional Experimental Details and Results}
\label{app:additional_results}
This appendix provides supplementary materials to further support the claims and findings of the paper, including detailed implementation specifics, additional visualizations, and expanded quantitative results.

\subsection{Implementation Details}
% TODO: Provide comprehensive details for full reproducibility:
% - VQVAE Architecture: Specify number of encoder/decoder residual blocks, attention layers, embedding dimension, number of codebook vectors, commitment loss weight, perceptual loss details if used.
% - VAR Transformer Architecture: Detail the number of layers, attention heads, embedding dimensions, context window length for tokens, specific type of positional embeddings (e.g., learned, sinusoidal) and how scale information is incorporated (e.g., separate scale embeddings, prepended scale tokens).
% - Training Hyperparameters: For both VQVAE and VAR Transformer, list batch size (global and per-device), learning rate schedule (e.g., cosine decay with warmup), optimizer (e.g., AdamW with specific beta values, weight decay), number of training epochs/steps, gradient accumulation steps, mixed precision training details (e.g., bfloat16). Specify the type and number of GPUs used for training (e.g., 128 A100 GPUs).
% - AR Baseline Construction: Detail the architecture of the AR baseline used for comparison. Specify its VQVAE stage (if it uses one, or if it's pixel-based), transformer size, and training procedure to ensure fairness of comparison with VAR.
% - Zero-shot Task Setup: For in-painting, describe the mask generation process (e.g., random rectangular masks, free-form masks from standard datasets) and mask ratios. For out-painting, specify the extension ratio and direction. For editing, detail the protocol for selecting regions and applying modifications.
% - Evaluation Protocols: For FID/IS, confirm the number of samples used (e.g., 50K) and the reference distribution (e.g., ImageNet validation set). For Precision/Recall, specify the k-NN value used.

\subsection{Visualizations of Generated Samples}
% TODO: Include several figures showing a diverse set of high-quality samples generated by VAR for ImageNet 256x256 and 512x512, covering a range of classes.
% TODO: Include figures providing direct qualitative comparisons against DiT and the AR Baseline for selected challenging examples or classes.
% Example:
% \begin{figure}[h]
% \centering
% \includegraphics[width=\textwidth]{figures/qualitative_comparison_imagenet.pdf} % Replace with your actual figure file
% \caption{Qualitative comparison of samples generated by VAR (ours), DiT, and AR Baseline on ImageNet 256x256 for diverse classes. VAR samples often exhibit better detail and global coherence.}
% \label{app_fig:qualitative_samples}
% \end{figure}

\subsection{Detailed Plots for Scaling Law Analysis}
% TODO: Provide detailed plots for the scaling law analysis (Section \ref{ssec:scaling_laws}).
% - Plot for FID vs. training data size (e.g., 10%, 25%, 50%, 100% of ImageNet), showing VAR and AR Baseline.
% - Plot for Test Loss (or validation perplexity/NLL) vs. Model Parameters (e.g., from 100M to X Billion parameters), clearly showing data points for each trained VAR model variant and the power-law fit line with its equation or R^2 value.
% - Consider adding scaling plots for IS or other metrics if insightful.
% Example:
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.7\textwidth]{figures/model_param_scaling_detailed.pdf} % Replace with your actual figure file
% \caption{Detailed plot of validation loss as a function of VAR model parameters, showing individual model configurations and the power-law fit. The x-axis should be log-scale for parameters, and y-axis log-scale for loss to show a linear trend if power-law holds.}
% \label{app_fig:model_scaling_detailed}
% \end{figure}


\subsection{Quantitative Results for Zero-Shot Tasks}
Detailed quantitative metrics for zero-shot tasks are presented here.
% TODO: Fill in with actual tables and quantitative results. Ensure comparison with relevant (ideally unsupervised or zero-shot) baselines for these tasks.
\subsubsection{In-painting Performance on Standard Benchmarks}
% Example Table Structure:
\begin{table}[h!]
\centering
\caption{Quantitative results for in-painting on Places2 \cite{zhou2017places} validation set (e.g., center masks 128$\times$128 on 256$\times$256 images) and ImageNet validation set (e.g., random irregular masks, 40-50\% area).}
\label{app_tab:inpainting_results}
\begin{tabular}{llccc}
\toprule
Dataset & Model & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS \cite{zhang2018unreasonable} $\downarrow$ \\ % Add LPIPS if measured
\midrule
Places2 & VAR (Ours, Zero-shot) & XX.X & X.XXX & Y.YYY \\
& Baseline Method A (Unsupervised) & AA.A & A.AAA & B.BBB \\
& Baseline Method B (Supervised) & CC.C & C.CCC & D.DDD \\
\midrule
ImageNet & VAR (Ours, Zero-shot) & XX.X & X.XXX & Y.YYY \\
& Baseline Method C (Unsupervised) & AA.A & A.AAA & B.BBB \\
\bottomrule
\end{tabular}
\end{table}
% TODO: Add \label{app_fig:inpainting_examples} and corresponding figure with qualitative examples.

\subsubsection{Out-painting Metrics}
% Example: Discussion of FID scores on extended image regions, user study results on coherence and plausibility.
% \begin{table}[h!]
% \centering
% \caption{Quantitative results for out-painting (e.g., extending ImageNet images by 50\% on each side).}
% \label{app_tab:outpainting_results}
% \begin{tabular}{lcc}
% \toprule
% Model & FID on Extended Region $\downarrow$ & User Preference (\%) $\uparrow$ \\
% \midrule
% VAR (Ours, Zero-shot) & XX.X & YY\% \\
% Baseline Method D (Zero-shot) & AA.A & BB\% \\
% \bottomrule
% \end{tabular}
% \end{table}
% TODO: Add \label{app_fig:outpainting_examples} and corresponding figure with qualitative examples.

\subsubsection{Image Editing Capabilities}
% Example: If quantitative metrics are hard, provide structured qualitative examples with clear "before" and "after" images for specific edits (e.g., object removal, attribute change).
% TODO: Add \label{app_fig:editing_examples} and corresponding figure with qualitative examples.


\subsection{Detailed Ablation Study Results}
% TODO: Provide tables summarizing the results of ablation studies mentioned in Section \ref{ssec:ablation}.
% Example Table Structure for Scale Selection:
\begin{table}[h!]
\centering
\caption{Detailed ablation study on scale selection strategy, number of scales, and VQVAE quality on ImageNet 256$\times$256. Main model uses exponential progression with 10 scales and VQVAE-A.}
\label{app_tab:ablation_details}
\begin{tabular}{llccc}
\toprule
Ablation Target & Configuration & FID $\downarrow$ & IS $\uparrow$ & Inference Speed (img/s) $\uparrow$ \\
\midrule
Scale Progression & Exponential (10 scales) & \textbf{1.73} & \textbf{350.2} & 12.5 \\
                  & Linear (10 scales)      & 2.15 & 330.1 & 12.3 \\
                  & Fewer Steps (e.g., 6 exp. scales) & 2.58 & 315.6 & 15.1 \\
\midrule
Number of Scales  & 8 Exponential Scales    & 1.95 & 340.5 & 14.0 \\
(Exponential)     & 10 Exponential Scales   & \textbf{1.73} & \textbf{350.2} & 12.5 \\
                  & 12 Exponential Scales   & 1.70 & 353.1 & 10.8 \\
\midrule
VQVAE Quality     & VQVAE-A (Recon FID 2.1) & \textbf{1.73} & \textbf{350.2} & 12.5 \\
                  & VQVAE-B (Recon FID 3.5) & 2.90 & 305.7 & 12.5 \\
% Add ablations on transformer depth/width if space permits or if particularly insightful
\bottomrule
\end{tabular}
\end{table}

% --- Bibliography ---
\bibliographystyle{unsrtnat} % Example style, check NeurIPS guidelines for the current year
\bibliography{references} % Assumes references.bib file in the same directory

\end{document}
```

And the example `references.bib` file (you **must** create and populate this with all your actual citations):


```bibtex
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@article{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  journal={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

@inproceedings{oord2016pixel,
  title={Pixel recurrent neural networks},
  author={Van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1747--1756},
  year={2016},
  organization={PMLR}
}

@article{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  journal={arXiv preprint arXiv:1802.05751},
  year={2018}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  booktitle={Advances in neural information processing systems},
  pages={6306--6315},
  year={2017}
}

@inproceedings{razavi2019generating,
  title={Generating diverse high-fidelity images with VQ-VAE-2},
  author={Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10719--10729}, % Note: Original prompt had 10719--10729, common error is to copy page numbers from other entries. This seems more plausible.
  year={2019}
}

@article{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={IEEE}
}

@inproceedings{oord2016conditional,
  title={Conditional image generation with pixelcnn decoders},
  author={Van den Oord, Aaron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  booktitle={Advances in neural information processing systems},
  pages={4790--4798},
  year={2016}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014} % Note: Original prompt had 2020, but Goodfellow GANs is 2014.
}

@inproceedings{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456}, % Or International Conference on Learning Representations (ICLR) 2021
  year={2020} % arXiv year
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric A and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@inproceedings{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  booktitle={Advances in neural information processing systems},
  pages={2234--2242},
  year={2016}
}

@inproceedings{kynkaanniemi2019improved,
  title={Improved precision and recall metric for assessing generative models},
  author={Kynk{\"a}{\"a}nniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32}, % NIPS 2019 is volume 32
  pages={8142--8152}, % Corrected page numbers
  year={2019}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}

@inproceedings{zhou2017places,
  title={Places: A 10 million image database for scene recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  booktitle={IEEE transactions on pattern analysis and machine intelligence}, % This is a journal, but often cited from its CVPR origins
  volume={40},
  number={6},
  pages={1452--1464},
  year={2017}, % The T-PAMI paper is 2017, original Places CVPR 2014
  publisher={IEEE}
}
